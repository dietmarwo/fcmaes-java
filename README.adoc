:encoding: utf-8
:imagesdir: img
:cpp: C++

= fcmaes-java - a Java gradient-free optimization library

fcmaes-java provides fast {cpp}/Eigen based implementations of its gradient free optimization algorithms.
It supports parallel optimization and parallel function evaluation. A new coordinated parallel retry mechanism 
can be viewed as a meta algorithm operating on a population of user defined optimization runs. 
fcmaes-java provides the same functionality as its Python variant https://github.com/dietmarwo/fast-cma-es[fcmaes].
It was used by the team Jena & Wuhan for the 11th China Trajectory Optimization Competition https://ctoc11.skyeststudio.com/[CTOC11] 
see https://github.com/dietmarwo/fcmaes-java/blob/master/CTOC11.adoc[fcmaes for CTOC11].

=== Features

- fcmaes-java is focused on optimization problems hard to solve.
- Minimized algorithm overhead - relative to the objective function evaluation time - even for high dimensions. 
- Parallel coordinated retry of sequences and random choices of optimization algorithms. 
- Parallel function evaluation.
- New DE (differential evolution) variant optimized for usage with parallel coordinated retry.
- GCL-DE (differential evolution) variant from Mingcheng Zuo.
- Fast C++ implementations of CMA-ES, differential evolution, dual annealing and the Harris hawks algorithm.
- Supports Linux and Windows
 
=== Compilation
 
* `mvn install`

=== Usage

See the F8 example https://github.com/dietmarwo/fcmaes-java/blob/master/src/main/java/fcmaes/examples/F8.java[F8.java]. 
Many other examples can be found in the test cases 
https://github.com/dietmarwo/fcmaes-java/blob/master/src/test/java/fcmaes/core/OptimizerTest.java[OptimizerTest.java].

=== Dependencies

Runtime:

- see https://github.com/dietmarwo/fcmaes-java/blob/master/pom.xml

Compile time (binaries for Linux and Windows are included):

- Eigen https://gitlab.com/libeigen/eigen (version >= 3.9 is required for CMA).
- pcg-cpp: https://github.com/imneme/pcg-cpp - used in all {cpp} optimization algorithms.
- LBFGSpp: https://github.com/yixuan/LBFGSpp/tree/master/include - used for dual annealing local optimization.
- Ascent: https://github.com/AnyarInc/Ascent/tree/master/include - used for fast ODE integration

=== Performance

On a single 16 core AMD 5950x CPU using the parallel coordinated retry mechanism 
solves ESAs 26-dimensional https://www.esa.int/gsp/ACT/projects/gtop/messenger_full/[Messenger full] problem
in about 40 minutes on average. The Messenger full benchmark models a
multi-gravity assist interplanetary space mission from Earth to Mercury. In 2009 the first good solution (6.9 km/s)
was submitted. It took more than five years to reach 1.959 km/s and three more years until 2017 to find the optimum 1.958 km/s. 
The picture below shows the progress of the whole science community since 2009:

image::Fsc.png[]  

For comparison: http://www.midaco-solver.com/data/pub/PDPTA20_Messenger.pdf[MXHCP paper] shows that using 1000 cores of the the 
Hokudai Supercomputer using Intel Xeon Gold 6148 CPUâ€™s with a clock rate of 2.7 GHz Messenger Full can be solved 
in about 1 hour using the MXHCP algorithm. 

79 runs of the coordinated parallel retry were performed on a single AMD 5950x CPU 
using the DE->CMA sequence as optimization algorithm: 

image::DECMA6.png[]

41 of the 79 runs reached a good result below 2 km/s:

image::DECMA2.png[]  

This is the best result for a single CPU reported so far, even faster than the old result for a 1000 core cluster referenced above. 
About 1.4*10^6 function evaluations per second were performed which shows excellent scaling of the algorithm utilizing all
16 cores / 32 threads.   